## Bagging
典型：随机森林。
有放回的采样数据，然后建模。
每次采样建模一次，且建的是同种模型。
预测结果按所有模型的预测结果进行投票（分类）/取平均（回归）

## Boosting
典型：adaboost，GBDT，Xgboost
多个弱分类器合在一起成为一个强分类器。
每个模型的训练与之前的训练结果有关系，模型的训练是顺序的。

## Bagging 与 Boosting的区别
讲完Boosting，Bagging，我们来总结下这两种框架算法的异同点： 
1）样本选择上： 
Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

2）样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

3）预测函数：

Bagging：所有预测函数的权重相等。

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

4）并行计算：

Bagging：各个预测函数可以并行生成

Boosting：理论上各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。计算角度来看，两种方法都可以并行。bagging, random forest并行化方法显而意见。boosting有强力工具stochastic gradient boosting

5）bagging是减少variance，而boosting是减少bias 
在机器学习中，我们用训练数据集去训练（学习）一个model（模型），通常的做法是定义一个Loss function（误差函数），通过将这个Loss（或者叫error）的最小化过程，来提高模型的性能（performance）。然而我们学习一个模型的目的是为了解决实际的问题（或者说是训练数据集这个领域（field）中的一般化问题），单纯地将训练数据集的loss最小化，并不能保证在解决更一般的问题时模型仍然是最优，甚至不能保证模型是可用的。这个训练数据集的loss与一般化的数据集的loss之间的差异就叫做generalization error。而generalization error又可以细分为Bias和Variance两个部分。 
即error=Bias+Variance 
。。。懒得抄了，看[这篇](https://blog.csdn.net/mr_tyting/article/details/72957853)，除了stacking的地方都写的不错

## stacking
看[这篇](https://zhuanlan.zhihu.com/p/26890738)
大概就是说，首先我有三种数据，1.训练集 2.测试集 3.需要预测的数据
然后stacking做的事情就是训练两层模型。
第一层是以类似于K折交叉验证的方式，使用训练集来训练K个模型（比如K个随机森林）
然后因为是K折验证嘛，每次会有其中的一折作为测试集来进行验证嘛，K个模型跑下来其实就相当于对训练集的每一个样本做一次预测。于是这里就得到了对训练集的预测结果TR_X，然后训练集本身来说，本来就有个标签，命名为TR_Y
接着，还是第一层，我们之前不是训练了K个模型吗，我们不是有测试集吗，全量的测试集在每个模型里都过一遍，能够得到K分对测试集的预测数据，把对应的数据相加然后取平均（除以K），这样就相当于得到了一份对测试集的预测数据，称之为TE_X，然后我们测试集不是本来就有个标签吗，称之为TE_Y
现在到了第二层------等一下，其实还没到第二层
刚才在第一层里，我们只训练了K个随机森林模型，此处我们还可以用同样的方法，训练K个逻辑回归，训练K个xgboost。。。。等等等，啥都行，设一共用了M种模型。
于是得到了有TR_X1, TR_Y1, TE_X1, TE_Y1, TR_X2, TR_Y2.........你懂的
现在到了**真**第二层，第二层里首先我们要做的是把之前用N种模型训练的。。。
编不下去了，网上的资料多而乱，并且对第二层都没有一个很细节的说明。只能在这里列举一个第二层的可行方案。
首先把之前的TR_Y1, TR_Y2, TR_Y3......加起来取平均，变成一列TR_Y
嗯，然后对TE_Y也这么做。这样就得到了一列train data的label和一列test data的label。
把TR_X1, TR_X2......按列拼接起来，变成M列，TE_X1,TE_X2....也拼起来变成M列。
这样就有了完整的新的train data和test data。
接下来就找个模型来训练吧。

然后做预测的时候，把上面的流程都走一遍。