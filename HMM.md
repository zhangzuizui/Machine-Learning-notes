#HMM
HMM在我看来是一个使用EM算法求解参数的典型模型。
HMM很重要的地方是两个假设：
1. **齐次马尔科夫性假设**：即假设状态的转移只与上次观测的结果有关
2. **观测独立性假设**：即本次观测的结果只与当前状态有关

其关键参数$\lambda$有三个
1. 初始状态概率向量$\pi$
2. 转移矩阵$A$，指对于上一个时间的观测，在本时间转移到各状态的概率
3. 观测矩阵$B$，指对于当前状态，观测到当前时间的各观测的概率

对于HMM，衍生出了三种问题。
1. 计算问题
    给定$\lambda$，计算观测到序列$O$的概率。
2. 学习问题
    给定观测序列$O$，求观测到该序列的参数$\lambda$，显然是一个极大似然估计的问题，用EM算法可解
3. 预测问题
    给定$O$和$\lambda$，预测给定观测下最有可能的状态序列$I$，即求$P(I|O)$（标注问题的核心，划重点）

## 计算问题
HMM的计算问题使用的是动态规划的方法，分为前向算法和后向算法。
### 前向算法
那么如何使用前向算法对目标问题进行求解？使用动态规划的方法求解问题其实就是将问题拆分后想办法进行迭代。那么假设我们需要求$t$时刻的观测$o_t$，那么就需要知道$t$时刻的状态，而$t$时刻的状态其实是由$t-1$时刻的观测情况决定的，这就形成了一个闭环。于是令$\alpha_t(i)$为$t$时刻状态为$q_i$且观测到当前观测序列的概率，即$\alpha_t(i)=P(o_1,o_2,...,o_t,i=q_i|\lambda)$
$$
    \begin{aligned}
        \alpha_1(i)&=\pi_ib_i(o_1) \\
        \alpha_t(i)&=(\sum_{j=1}^N\alpha_{t-1}(j)a_{ji})b_i(o_t)
    \end{aligned}
$$
最后得到
$$P(O|\lambda)=\sum_{i=1}^N\alpha_t(i)$$

### 后向算法
后向算法与前向算法类似的
$$\beta_t(i)=(\sum_{j=1}^Na_{ij}b_j(o_{t+1}))\beta_{t+1}(i)$$
其中，$t=T$时
$$\beta_T(i)=1$$

### 前后向混合计算
$$P(O|\lambda)=\sum_{i=1}^N\sum_{j=1}^N\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(i)$$

## 学习算法
### 监督学习方法
jieba分词用的就是这种方法，用的数据是公开的1988年人民日报数据。
这个方法说是监督学习，实际上就是个简单的统计，根据已有的数据统计一下，然后认为统计结果服从实际分布。

### 非监督学习方法
HMM的非监督学习算法叫做Baun-Welch算法，实际上就是个EM算法。
书上的推导过程十分清楚，懒得写了P181

## 预测算法
在已经$\pi$和$O$的情况下求$I$也是用动态规划的方法，被称为维特比算法，在形式上很像最短路径算法Dijkstra

首先是要储存$t$时刻每种状态的最大概率的情况下观测到$o_t$的概率
令$\delta_0(i)=\pi_i$
那么有
$$\delta_{t+1}(i)=(\underset{j}\max \delta_t(j)a_{ji})b_i(o_{t+1})$$
再同时存下到当前时刻，每种状态的转移路径。
最后得到$\delta_T$后，找到概率最大的那个状态，然后就确认了这个转移路径。

## jieba分词原理
其实原理很简单，分词实际上就是文本标注问题，把每个字标记为{B,M,E,S}四种属性，即begin,middle,end,single.先根据1988年人民日报，得到了参数。然后jieba自建有一个带有词频的词典库。在分词的时候先以字典树（Trie Tree）的形式对词典库建树，然后根据词频，将句子中词典库里记录了的词进行分词，对没记录的词将其按字分开，并记录下这些没记录的词的位置。
然后通过已经得到了的参数，根据维特比算法，计算出每个字的状态，最后根据状态将字组合成词。
jieba的词性识别应该也是差不多的原理。