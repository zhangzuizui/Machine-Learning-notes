# HMM
HMM在我看来是一个使用EM算法求解参数的典型模型。
HMM很重要的地方是两个假设：
1. **齐次马尔科夫性假设**：即假设状态的转移只与上次观测的结果有关
2. **观测独立性假设**：即本次观测的结果只与当前状态有关

其关键参数$\lambda$有三个
1. 初始状态概率向量$\pi$
2. 转移矩阵$A$，指对于上一个时间的观测，在本时间转移到各状态的概率
3. 观测矩阵$B$，指对于当前状态，观测到当前时间的各观测的概率

对于HMM，衍生出了三种问题。
1. 计算问题
    给定$\lambda$，计算观测到序列$O$的概率。
2. 学习问题
    给定观测序列$O$，求观测到该序列的参数$\lambda$，显然是一个极大似然估计的问题，用EM算法可解
3. 预测问题
    给定$O$和$\lambda$，预测给定观测下最有可能的状态序列$I$，即求$P(I|O)$（标注问题的核心，划重点）

## 计算问题
HMM的计算问题使用的是动态规划的方法，分为前向算法和后向算法。
### 前向算法
那么如何使用前向算法对目标问题进行求解？使用动态规划的方法求解问题其实就是将问题拆分后想办法进行迭代。那么假设我们需要求$t$时刻的观测$o_t$，那么就需要知道$t$时刻的状态，而$t$时刻的状态其实是由$t-1$时刻的观测情况决定的，这就形成了一个闭环。于是令$\alpha_t(i)$为$t$时刻状态为$q_i$且观测到当前观测序列的概率，即$\alpha_t(i)=P(o_1,o_2,...,o_t,i=q_i|\lambda)$
$$
    \begin{aligned}
        \alpha_1(i)&=\pi_ib_i(o_1) \\
        \alpha_t(i)&=(\sum_{j=1}^N\alpha_{t-1}(j)a_{ji})b_i(o_t)
    \end{aligned}
$$
最后得到
$$P(O|\lambda)=\sum_{i=1}^N\alpha_t(i)$$

### 后向算法
后向算法与前向算法类似的
$$\beta_t(i)=(\sum_{j=1}^Na_{ij}b_j(o_{t+1}))\beta_{t+1}(i)$$
其中，$t=T$时
$$\beta_T(i)=1$$

### 前后向混合计算
$$P(O|\lambda)=\sum_{i=1}^N\sum_{j=1}^N\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(i)$$

## 学习算法
### 监督学习方法
jieba分词用的就是这种方法，用的数据是公开的1988年人民日报数据。
这个方法说是监督学习，实际上就是个简单的统计，根据已有的数据统计一下，然后认为统计结果服从实际分布。

### 非监督学习方法
HMM的非监督学习算法叫做Baun-Welch算法，实际上就是个EM算法。
书上的推导过程十分清楚，懒得写了P181

## 预测算法
在已经$\pi$和$O$的情况下求$I$也是用动态规划的方法，被称为维特比算法，在形式上很像最短路径算法Dijkstra

首先是要储存$t$时刻每种状态的最大概率的情况下观测到$o_t$的概率
令$\delta_0(i)=\pi_i$
那么有
$$\delta_{t+1}(i)=(\underset{j}\max \delta_t(j)a_{ji})b_i(o_{t+1})$$
再同时存下到当前时刻，每种状态的转移路径。
最后得到$\delta_T$后，找到概率最大的那个状态，然后就确认了这个转移路径。

## jieba分词原理
其实原理很简单，分词实际上就是文本标注问题，把每个字标记为{B,M,E,S}四种属性，即begin,middle,end,single.先根据1988年人民日报，得到了参数。然后jieba自建有一个带有词频的词典库。在分词的时候先以字典树（Trie Tree）的形式对词典库建树，然后根据词频，将句子中词典库里记录了的词进行分词，对没记录的词将其按字分开，并记录下这些没记录的词的位置。
然后通过已经得到了的参数，根据维特比算法，计算出每个字的状态，最后根据状态将字组合成词。
jieba的词性识别应该也是差不多的原理。

# CRF
CRF其实跟HMM真的是特别相似，只是在对于问题做假设的之后有一点不同。就是因为HMM与CRF的区别，使我从本质上弄懂了什么叫做生成模型，什么叫做判别模型。
对于HMM来说，有两个假设，一是齐次马尔科夫性假设，二是观测独立性假设。 
对于CRF来说，相当于是有一个假设，CRF即在给定随机变量X的条件下，随机变量Y的马尔科夫随机场，而马尔科夫随机场值的是无向图中联合概率分布P(Y)满足成对、局部或全局马尔科夫性。
直接来看线性链条件随机场的表达式
$$p(y|x)=\frac{1}{Z(x)}\exp(\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i),\sum_{i,l}\mu_ls_l(y_i,x,i))$$
其中，
$$Z(x)=\sum_y\exp(\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i),\sum_{i,l}\mu_ls_l(y_i,x,i))$$
从这个表达式可以看出什么？
1. 表达式中不含有隐变量
2. 某个状态的概率主要只与3个变量有关：上一个状态的y_{i-1}，当前状态的条件x_i，与当前状态的y_i
3. 它是直接求解$y$的条件概率，是个判别模型

那么问题来了，HMM不是也是求概率吗？比如说$P(I|O)$，这不也是条件概率吗，为什么说HMM是生成模型？这一点用前向-后向算法来举例简直十分明了。
类似于HMM的，把CRF的参数也表示成矩阵的形式，那么可以理解为得到了$n$个转移矩阵$M_i$
**CRF的前向算法是**：
$$\alpha_i(x)=\alpha_{i-1}(x)M_i(x)$$
**CRF的后向算法是**：
$$\beta_i(x)=M_{i+1}(x)\beta_{i+1}(x)$$
与HMM对比一下，少了一个很关键的东西，一个求和符号。因为HMM的参数表示的是数据的分布，是观测值与隐变量的联合概率分布，就相当于
$$P(y)=\sum_xP(x,y)$$
即对于生成模型来说，我们想要知道的是，数据的分布情况是怎样的，而对于CRF来说，其参数表示的是“判断的方式”，即对于判别模型，我们想要知道的是，在什么情况下我可以得到这个值。
## CRF的学习算法
因为CRF是判别模型，不会含有说隐变量这种东西，因此学习方法跟HMM不同，不是使用EM，而是使用IIS，拟牛顿法等。
## CRF的预测算法
还是维特比算法，与HMM的写法不一致的是，这里的维特比算法几乎完全可以理解成是一个dijkstra，具体的，看CRF的表达式，我们要求的标记序列的概率，是以$e$为底的指数相乘形式，最后将表现为幂相加。因此如李航《统计学习方法》P206所示，动态规划过程是以加法形式实现的，这完全可以理解求无向图（有向图）中两点间的最短距离，并输出该路径。