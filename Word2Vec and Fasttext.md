#Word2Vec
word2vec的难点就在于tricks有很多（其实也没多少，也就俩？）这篇[博文](https://blog.csdn.net/a819825294/article/details/52438625)里，对word2vec已经将的很清楚了，清楚的不能再清楚了，所以下面只做一些总结性的表述。
## Hierarchical Softmax
1. 没什么标题
其实吧我觉得这里这个softmax实在是有点唬人，虽然说sigmoid函数就是由二元的softmax推出来的，但是实际上对于哈夫曼树结构的输出来说，其实是一直在用sigmoid进行多次二分类。需要强调的是，在上面的文章中看起来好像是只对n-grams这个窗口内的词做了一个哈夫曼树，然后对中心词（CBOW）/周围词（Skip-gram）进行预测，实则不然，这一点在文章中也有提到，是对语料库$C$中的所有词，根据其每个词的词频建了一颗哈夫曼树，该树的叶子结点数等于词典库$D$的总词数$|D|$。
2. 有悖于常理之处
一般情况下，对于逻辑回归我们对于标签的正类是定义为1，负类是定义为0，于是似然函数大概是长这样
$$L=\sum_{i=1}^N\sigma(w^Tx_i)^{y_i}(1-\sigma(w^Tx_i))^{1-y_i}$$
嗯，你们懂的，再求个对数实际上就是交叉熵
但是！word2vec里面就写得让人很难受，它是把正类定义为0，负类定义为1，所以似然函数变成了大概长这样
$$L=\sum_{i=1}^N\sigma(w^Tx_i)^{1-y_i}(1-\sigma(w^Tx_i))^{y_i}$$
好吧，其实这不重要，只是一种标识而已，怎么样都行。不过在word2vec的源码中是使用的第二种方式进行标识，所以强调的写一下避免搞迷糊。
3. 迭代过程（简单的计算）
接着计算的过程实际上是非常非常非常简单，就是一个无约束的凹优化问题，与常规问题不同的是，在这个问题中不光只有权重$w$是参数，$x$，也就是输入（CBOW问题中对应的是周围词的词向量累加后的平均，Skip-gram中对应的是中心词的词向量）也是参数，是可以改变的（毕竟我们要求的就是$x$）。还有就是不能让对$w$的修改影响到当前迭代中$x$的修改，也就是说对$w$求完梯度后，要将这个中间结果（梯度）暂存起来，然后再改变$w$，在$w$修正完后，再通过中间结果去改变$x$。
4. 梯度上升
因为在向量的训练过程中，其实质是最大化对数似然函数，对于凹函数来说当然是“梯度上升”而不是“梯度下降”，所以注意在迭代过程中不是减去学习率乘梯度，而是加。
## Negative sampling
负采样与分级softmax差不多（计算过程），都是一个凹函数的求解问题，这里不说了，算法流程几乎一模一样。需要讨论的就是如何对样本进行负采样。
1. Skip-gram代码中的采样简化
文中也提到了这一点，就是如果对于推导的公式来说的话，相当于是每次求解周围词的向量时，都要对每个周围词进行一次采样。实际上并不是这样，在代码实现中，可以理解为是事先对所有的中心词，做|周围词词数|次数的负采样，这在文章中也是有证明的。其实这一点也是十分好懂，这样相当于是避免了同一位置的词的重复采样。
2. 采样策略
文中也说得很清楚，就是一个带权采样的过程，根据词频对词进行采样，词频越高被采样到的几率也越高。当然如果是偶然采样到了本应该是正样本的样本，那么重新做采样就是了。
## 其他tricks
word2vec的其他tricks都是用来加速词向量的训练的。
### 近似sigmoid函数计算
这一点是十分容易理解的，因为sigmoid函数的梯度函数是一个类似于高斯分布的图像，在两边的时候其梯度变化十分小，几乎为0，所以当$x$的值足够小/足够大时，直接认为$\sigma(x)=0/1$从而省去了计算过程。
### 对高频词低频词的处理
因为词频太高的词或者词频太低的词，对于整个词向量的训练来说其影响是十分的小的，因此有策略是将低频次直接舍弃，将高频词以词频越高概率越高的几率进行舍弃，以此来加快训练速度。
# Fasttext
emmmm我只稍微瞅了一眼源码，可能有不完善的地方。
总之实际上就是word2vec的基础上+了一个trick叫做word embedding。
嗯，其实我这个是有点懵的。
1. 论文里提到的
就是因为训练词向量的时候，实则还是word2vec的那个方式，把词向量累加起来平均了就丢到模型里去了，这样做就有个问题了。我训练这个词向量的目的是要用来做文本分类，但是如果不考虑语序的话，你分出来的类肯定是会出现问题的，所以有个trick就是把相邻的几个词给组合到一起，然后丢到模型里跑，比如
我 爱 中国，然后我设置这个embedding的参数为2，就变成了 我 爱 中国 我爱 爱中国
然后再丢进去跑
2. 论文里好像没提到的
反正我是没看到这。。。不过十分矛盾的是，fasttext的python包里给的接口并没有给出我上面说到的1里面的那种embedding，而是仿佛只给了现在将要讲到的一种叫word grams的东西（不确定）。就是比如有个单词"delicious"，通过取word grams之后能衍生出"delic"， "delici"。。。。总之就是把词拆开了然后挨个添加到句子里去，具体怎么拆的忘了，也没找到相关的说明，不过确实看到有人讲过这个。大概就是通过这种方式考虑了词缀的一些影响，很可惜的是中文没法使用这个功能。
## fasttext训练
词向量的训练跟word2vec完全相同（除了上面说的增加的tricks），然后训练好词向量之后总之就是把整个句子做做word embedding啥的然后全部映射成向量，累加取平均丢到softmax里去跑，就是个多分类任务。把训练词向量的过程当做是特征工程就好。